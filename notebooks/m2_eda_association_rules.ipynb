{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# M2: Exploratory Data Analysis & Association Rule Mining\n",
    "## Personal Strength Training Data\n",
    "\n",
    "**Discovery Question Addressed:** What exercises are frequently performed together within the same workout session, and what compound movement patterns emerge across different training splits?\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'mlxtend'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 9\u001b[39m\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mdatetime\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m datetime\n\u001b[32m      8\u001b[39m \u001b[38;5;66;03m# Association rule mining\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmlxtend\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mfrequent_patterns\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m apriori, association_rules\n\u001b[32m     10\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmlxtend\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpreprocessing\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m TransactionEncoder\n\u001b[32m     12\u001b[39m \u001b[38;5;66;03m# Settings\u001b[39;00m\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'mlxtend'"
     ]
    }
   ],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "\n",
    "# Association rule mining\n",
    "from mlxtend.frequent_patterns import apriori, association_rules\n",
    "from mlxtend.preprocessing import TransactionEncoder\n",
    "\n",
    "# Settings\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 100)\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "sns.set_palette('husl')\n",
    "\n",
    "print(\"Libraries loaded successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load datasets\n",
    "# Update these paths to match your file locations\n",
    "sessions_df = pd.read_csv('training_sessions.csv')\n",
    "sets_df = pd.read_csv('training_sets.csv')\n",
    "\n",
    "# Convert date columns\n",
    "sessions_df['date'] = pd.to_datetime(sessions_df['date'])\n",
    "sets_df['date'] = pd.to_datetime(sets_df['date'])\n",
    "\n",
    "print(f\"Sessions dataset: {sessions_df.shape[0]} rows, {sessions_df.shape[1]} columns\")\n",
    "print(f\"Sets dataset: {sets_df.shape[0]} rows, {sets_df.shape[1]} columns\")\n",
    "print(f\"\\nDate range: {sessions_df['date'].min().date()} to {sessions_df['date'].max().date()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Quality Assessment\n",
    "\n",
    "Before analysis, we need to understand and quantify data quality issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sessions dataset overview\n",
    "print(\"=\" * 60)\n",
    "print(\"SESSIONS DATASET QUALITY REPORT\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\nTotal sessions: {len(sessions_df)}\")\n",
    "print(f\"\\nMissing values per column:\")\n",
    "sessions_missing = sessions_df.isnull().sum()\n",
    "sessions_missing_pct = (sessions_missing / len(sessions_df) * 100).round(2)\n",
    "for col in sessions_df.columns:\n",
    "    if sessions_missing[col] > 0:\n",
    "        print(f\"  {col}: {sessions_missing[col]} ({sessions_missing_pct[col]}%)\")\n",
    "if sessions_missing.sum() == 0:\n",
    "    print(\"  No missing values\")\n",
    "\n",
    "print(f\"\\nSynthetic sessions: {sessions_df['is_synthetic'].sum()} ({sessions_df['is_synthetic'].mean()*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sets dataset quality\n",
    "print(\"=\" * 60)\n",
    "print(\"SETS DATASET QUALITY REPORT\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\nTotal sets: {len(sets_df)}\")\n",
    "\n",
    "print(f\"\\nMissing values per column:\")\n",
    "sets_missing = sets_df.isnull().sum()\n",
    "sets_missing_pct = (sets_missing / len(sets_df) * 100).round(2)\n",
    "for col in sets_df.columns:\n",
    "    if sets_missing[col] > 0:\n",
    "        print(f\"  {col}: {sets_missing[col]} ({sets_missing_pct[col]}%)\")\n",
    "\n",
    "# Weight notation analysis\n",
    "print(f\"\\n--- Weight Data Quality ---\")\n",
    "missing_weight = sets_df['weight_lbs'].isnull().sum()\n",
    "print(f\"Sets with missing weight_lbs: {missing_weight} ({missing_weight/len(sets_df)*100:.1f}%)\")\n",
    "\n",
    "# Reps analysis\n",
    "missing_reps = sets_df['reps'].isnull().sum()\n",
    "print(f\"Sets with missing reps: {missing_reps} ({missing_reps/len(sets_df)*100:.1f}%)\")\n",
    "\n",
    "# Volume analysis\n",
    "missing_volume = sets_df['volume'].isnull().sum()\n",
    "print(f\"Sets with missing volume: {missing_volume} ({missing_volume/len(sets_df)*100:.1f}%)\")\n",
    "\n",
    "# Check for paused sets\n",
    "paused_sets = sets_df['notes'].str.contains('paused', case=False, na=False).sum()\n",
    "print(f\"\\nPaused sets: {paused_sets} ({paused_sets/len(sets_df)*100:.1f}%)\")\n",
    "\n",
    "# Bodyweight exercises\n",
    "bw_sets = sets_df['notes'].str.contains('bodyweight', case=False, na=False).sum()\n",
    "print(f\"Bodyweight sets: {bw_sets} ({bw_sets/len(sets_df)*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise name standardization check\n",
    "print(\"=\" * 60)\n",
    "print(\"EXERCISE STANDARDIZATION REPORT\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "n_raw_exercises = sets_df['exercise_raw'].nunique()\n",
    "n_standard_exercises = sets_df['exercise_standard'].nunique()\n",
    "\n",
    "print(f\"\\nUnique raw exercise names: {n_raw_exercises}\")\n",
    "print(f\"Unique standardized exercise names: {n_standard_exercises}\")\n",
    "print(f\"Reduction from standardization: {n_raw_exercises - n_standard_exercises} names consolidated ({(1 - n_standard_exercises/n_raw_exercises)*100:.1f}% reduction)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Exploratory Data Analysis\n",
    "\n",
    "### 3.1 Temporal Patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training frequency by day of week\n",
    "day_order = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']\n",
    "day_counts = sessions_df['day_of_week'].value_counts().reindex(day_order)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "bars = ax.bar(day_counts.index, day_counts.values, color=sns.color_palette('husl', 7))\n",
    "ax.set_xlabel('Day of Week')\n",
    "ax.set_ylabel('Number of Sessions')\n",
    "ax.set_title('Training Frequency by Day of Week')\n",
    "\n",
    "# Add value labels\n",
    "for bar, val in zip(bars, day_counts.values):\n",
    "    ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 1, \n",
    "            str(val), ha='center', va='bottom', fontsize=10)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('fig1_training_frequency_by_day.png', dpi=150)\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nMost common training day: {day_counts.idxmax()} ({day_counts.max()} sessions)\")\n",
    "print(f\"Least common training day: {day_counts.idxmin()} ({day_counts.min()} sessions)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training volume over time\n",
    "sessions_df['month'] = sessions_df['date'].dt.to_period('M')\n",
    "monthly_volume = sessions_df.groupby('month')['total_volume'].mean()\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(14, 5))\n",
    "monthly_volume.plot(kind='line', marker='o', ax=ax, linewidth=2, markersize=4)\n",
    "ax.set_xlabel('Month')\n",
    "ax.set_ylabel('Average Session Volume (lbs × reps)')\n",
    "ax.set_title('Average Training Volume Over Time')\n",
    "ax.tick_params(axis='x', rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.savefig('fig2_volume_over_time.png', dpi=150)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Days between sessions distribution\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "sessions_df['days_since_last'].hist(bins=20, ax=ax, edgecolor='black', alpha=0.7)\n",
    "ax.axvline(sessions_df['days_since_last'].median(), color='red', linestyle='--', \n",
    "           label=f\"Median: {sessions_df['days_since_last'].median():.0f} days\")\n",
    "ax.set_xlabel('Days Since Last Session')\n",
    "ax.set_ylabel('Frequency')\n",
    "ax.set_title('Distribution of Rest Days Between Sessions')\n",
    "ax.legend()\n",
    "plt.tight_layout()\n",
    "plt.savefig('fig3_rest_days_distribution.png', dpi=150)\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nRest days statistics:\")\n",
    "print(f\"  Mean: {sessions_df['days_since_last'].mean():.1f} days\")\n",
    "print(f\"  Median: {sessions_df['days_since_last'].median():.0f} days\")\n",
    "print(f\"  Max gap: {sessions_df['days_since_last'].max()} days\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Workout Type Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Workout type distribution\n",
    "workout_counts = sessions_df['workout_type'].value_counts()\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "bars = ax.barh(workout_counts.index[:15], workout_counts.values[:15])\n",
    "ax.set_xlabel('Number of Sessions')\n",
    "ax.set_ylabel('Workout Type')\n",
    "ax.set_title('Top 15 Workout Types by Frequency')\n",
    "ax.invert_yaxis()\n",
    "\n",
    "# Add value labels\n",
    "for bar, val in zip(bars, workout_counts.values[:15]):\n",
    "    ax.text(val + 0.5, bar.get_y() + bar.get_height()/2, \n",
    "            str(val), va='center', fontsize=9)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('fig4_workout_types.png', dpi=150)\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nTotal unique workout types: {sessions_df['workout_type'].nunique()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Volume by workout type\n",
    "workout_volume = sessions_df.groupby('workout_type')['total_volume'].agg(['mean', 'std', 'count'])\n",
    "workout_volume = workout_volume[workout_volume['count'] >= 5].sort_values('mean', ascending=False)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "bars = ax.barh(workout_volume.index[:12], workout_volume['mean'].values[:12])\n",
    "ax.set_xlabel('Average Total Volume (lbs × reps)')\n",
    "ax.set_ylabel('Workout Type')\n",
    "ax.set_title('Average Volume by Workout Type (min 5 sessions)')\n",
    "ax.invert_yaxis()\n",
    "plt.tight_layout()\n",
    "plt.savefig('fig5_volume_by_workout_type.png', dpi=150)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Exercise Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Most frequent exercises\n",
    "exercise_counts = sets_df['exercise_standard'].value_counts()\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 8))\n",
    "bars = ax.barh(exercise_counts.index[:20], exercise_counts.values[:20])\n",
    "ax.set_xlabel('Number of Sets')\n",
    "ax.set_ylabel('Exercise')\n",
    "ax.set_title('Top 20 Most Frequent Exercises')\n",
    "ax.invert_yaxis()\n",
    "plt.tight_layout()\n",
    "plt.savefig('fig6_top_exercises.png', dpi=150)\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nTotal unique exercises: {sets_df['exercise_standard'].nunique()}\")\n",
    "print(f\"\\nTop 10 exercises account for {exercise_counts.head(10).sum()/len(sets_df)*100:.1f}% of all sets\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Weight distribution by exercise category\n",
    "# Create exercise categories based on common patterns\n",
    "def categorize_exercise(name):\n",
    "    name_lower = name.lower()\n",
    "    if any(x in name_lower for x in ['bench', 'chest', 'fly', 'dip', 'press']):\n",
    "        if 'shoulder' in name_lower or 'overhead' in name_lower or 'ohp' in name_lower:\n",
    "            return 'Shoulders'\n",
    "        return 'Chest'\n",
    "    elif any(x in name_lower for x in ['row', 'pull', 'lat', 'pulldown', 'chin']):\n",
    "        return 'Back'\n",
    "    elif any(x in name_lower for x in ['squat', 'leg', 'lunge', 'hack', 'curl', 'extension', 'calf']) and 'curl' in name_lower and 'leg' in name_lower:\n",
    "        return 'Legs'\n",
    "    elif any(x in name_lower for x in ['squat', 'leg', 'lunge', 'hack', 'extension', 'calf', 'abduct']):\n",
    "        return 'Legs'\n",
    "    elif any(x in name_lower for x in ['curl', 'bicep', 'hammer', 'preacher']) and 'leg' not in name_lower:\n",
    "        return 'Biceps'\n",
    "    elif any(x in name_lower for x in ['tricep', 'pushdown', 'skull', 'extension']) and 'leg' not in name_lower:\n",
    "        return 'Triceps'\n",
    "    elif any(x in name_lower for x in ['deadlift', 'rdl', 'romanian']):\n",
    "        return 'Posterior Chain'\n",
    "    elif any(x in name_lower for x in ['lateral', 'delt', 'shoulder', 'shrug', 'upright']):\n",
    "        return 'Shoulders'\n",
    "    else:\n",
    "        return 'Other'\n",
    "\n",
    "sets_df['exercise_category'] = sets_df['exercise_standard'].apply(categorize_exercise)\n",
    "\n",
    "# Plot\n",
    "category_counts = sets_df['exercise_category'].value_counts()\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "colors = sns.color_palette('husl', len(category_counts))\n",
    "ax.pie(category_counts.values, labels=category_counts.index, autopct='%1.1f%%', \n",
    "       colors=colors, startangle=90)\n",
    "ax.set_title('Distribution of Sets by Muscle Group')\n",
    "plt.tight_layout()\n",
    "plt.savefig('fig7_muscle_group_distribution.png', dpi=150)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Session Characteristics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation matrix for session metrics\n",
    "session_metrics = sessions_df[['num_exercises', 'num_sets', 'total_volume', \n",
    "                                'avg_weight', 'avg_reps', 'session_duration_est', 'days_since_last']]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "correlation_matrix = session_metrics.corr()\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', center=0, \n",
    "            fmt='.2f', ax=ax, square=True)\n",
    "ax.set_title('Correlation Matrix: Session Metrics')\n",
    "plt.tight_layout()\n",
    "plt.savefig('fig8_correlation_matrix.png', dpi=150)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Session volume vs number of exercises\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "ax.scatter(sessions_df['num_exercises'], sessions_df['total_volume'], alpha=0.5)\n",
    "ax.set_xlabel('Number of Exercises')\n",
    "ax.set_ylabel('Total Volume')\n",
    "ax.set_title('Session Volume vs Number of Exercises')\n",
    "\n",
    "# Add trend line\n",
    "z = np.polyfit(sessions_df['num_exercises'], sessions_df['total_volume'], 1)\n",
    "p = np.poly1d(z)\n",
    "x_line = np.linspace(sessions_df['num_exercises'].min(), sessions_df['num_exercises'].max(), 100)\n",
    "ax.plot(x_line, p(x_line), \"r--\", alpha=0.8, label=f'Trend line')\n",
    "ax.legend()\n",
    "plt.tight_layout()\n",
    "plt.savefig('fig9_volume_vs_exercises.png', dpi=150)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Data Preprocessing for Association Rules\n",
    "\n",
    "For association rule mining, we need to transform our data into a transactional format where each session is a transaction and exercises are items."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create transactions from sessions\n",
    "# Each session becomes a transaction with exercises as items\n",
    "\n",
    "def parse_exercises(exercises_str):\n",
    "    \"\"\"Parse comma-separated exercise string into list\"\"\"\n",
    "    if pd.isna(exercises_str):\n",
    "        return []\n",
    "    return [ex.strip() for ex in exercises_str.split(',')]\n",
    "\n",
    "# Create list of transactions\n",
    "transactions = sessions_df['exercises_list'].apply(parse_exercises).tolist()\n",
    "\n",
    "print(f\"Total transactions (sessions): {len(transactions)}\")\n",
    "print(f\"\\nSample transactions:\")\n",
    "for i in range(3):\n",
    "    print(f\"  Session {i+1}: {transactions[i]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform to one-hot encoded format for mlxtend\n",
    "te = TransactionEncoder()\n",
    "te_array = te.fit_transform(transactions)\n",
    "transaction_df = pd.DataFrame(te_array, columns=te.columns_)\n",
    "\n",
    "print(f\"Transaction matrix shape: {transaction_df.shape}\")\n",
    "print(f\"  - {transaction_df.shape[0]} sessions\")\n",
    "print(f\"  - {transaction_df.shape[1]} unique exercises\")\n",
    "\n",
    "# Check sparsity\n",
    "sparsity = 1 - (transaction_df.sum().sum() / (transaction_df.shape[0] * transaction_df.shape[1]))\n",
    "print(f\"\\nMatrix sparsity: {sparsity*100:.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examine exercise frequency in transactions\n",
    "exercise_freq = transaction_df.sum().sort_values(ascending=False)\n",
    "exercise_support = exercise_freq / len(transaction_df)\n",
    "\n",
    "print(\"Top 15 exercises by support (frequency in sessions):\")\n",
    "print(\"=\"*50)\n",
    "for ex, sup in exercise_support.head(15).items():\n",
    "    print(f\"  {ex}: {sup:.3f} ({int(exercise_freq[ex])} sessions)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Association Rule Mining\n",
    "\n",
    "### 5.1 Parameter Selection\n",
    "\n",
    "**Minimum Support:** 0.05 (5%)\n",
    "- Rationale: With 419 sessions, this means an itemset must appear in at least ~21 sessions\n",
    "- This filters out very rare combinations while still capturing meaningful patterns\n",
    "\n",
    "**Minimum Confidence:** 0.5 (50%)\n",
    "- Rationale: If exercise A appears, we want at least 50% chance of seeing exercise B\n",
    "- This ensures rules have predictive value\n",
    "\n",
    "**Minimum Lift:** 1.0\n",
    "- Rationale: Lift > 1 indicates positive association (better than random chance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply Apriori algorithm\n",
    "MIN_SUPPORT = 0.05\n",
    "MIN_CONFIDENCE = 0.5\n",
    "MIN_LIFT = 1.0\n",
    "\n",
    "print(f\"Running Apriori with min_support={MIN_SUPPORT}...\")\n",
    "frequent_itemsets = apriori(transaction_df, min_support=MIN_SUPPORT, use_colnames=True)\n",
    "frequent_itemsets['length'] = frequent_itemsets['itemsets'].apply(len)\n",
    "\n",
    "print(f\"\\nFrequent itemsets found: {len(frequent_itemsets)}\")\n",
    "print(f\"\\nBreakdown by itemset size:\")\n",
    "print(frequent_itemsets['length'].value_counts().sort_index())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate association rules\n",
    "rules = association_rules(frequent_itemsets, metric='confidence', min_threshold=MIN_CONFIDENCE)\n",
    "rules = rules[rules['lift'] >= MIN_LIFT]\n",
    "\n",
    "print(f\"Association rules generated: {len(rules)}\")\n",
    "print(f\"\\nRules filtered by: confidence >= {MIN_CONFIDENCE}, lift >= {MIN_LIFT}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display top rules by lift\n",
    "rules_sorted = rules.sort_values('lift', ascending=False)\n",
    "\n",
    "print(\"\\nTOP 20 ASSOCIATION RULES (sorted by lift)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for idx, row in rules_sorted.head(20).iterrows():\n",
    "    antecedent = ', '.join(list(row['antecedents']))\n",
    "    consequent = ', '.join(list(row['consequents']))\n",
    "    print(f\"\\n{antecedent}\")\n",
    "    print(f\"  → {consequent}\")\n",
    "    print(f\"  Support: {row['support']:.3f} | Confidence: {row['confidence']:.3f} | Lift: {row['lift']:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize rules: Support vs Confidence colored by Lift\n",
    "fig, ax = plt.subplots(figsize=(12, 8))\n",
    "scatter = ax.scatter(rules['support'], rules['confidence'], \n",
    "                     c=rules['lift'], cmap='viridis', alpha=0.6, s=50)\n",
    "ax.set_xlabel('Support')\n",
    "ax.set_ylabel('Confidence')\n",
    "ax.set_title('Association Rules: Support vs Confidence (color = Lift)')\n",
    "plt.colorbar(scatter, label='Lift')\n",
    "plt.tight_layout()\n",
    "plt.savefig('fig10_rules_support_confidence.png', dpi=150)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze rules by lift distribution\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "rules['lift'].hist(bins=30, ax=ax, edgecolor='black', alpha=0.7)\n",
    "ax.axvline(rules['lift'].median(), color='red', linestyle='--', \n",
    "           label=f\"Median lift: {rules['lift'].median():.2f}\")\n",
    "ax.set_xlabel('Lift')\n",
    "ax.set_ylabel('Number of Rules')\n",
    "ax.set_title('Distribution of Lift Values Across Rules')\n",
    "ax.legend()\n",
    "plt.tight_layout()\n",
    "plt.savefig('fig11_lift_distribution.png', dpi=150)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Workout-Specific Association Rules\n",
    "\n",
    "Let's also analyze patterns within specific workout types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze \"Upper\" workout associations\n",
    "upper_sessions = sessions_df[sessions_df['workout_type'].str.contains('Upper|Torso', case=False, na=False)]\n",
    "upper_transactions = upper_sessions['exercises_list'].apply(parse_exercises).tolist()\n",
    "\n",
    "if len(upper_transactions) >= 20:  # Need enough sessions\n",
    "    te_upper = TransactionEncoder()\n",
    "    te_upper_array = te_upper.fit_transform(upper_transactions)\n",
    "    upper_df = pd.DataFrame(te_upper_array, columns=te_upper.columns_)\n",
    "    \n",
    "    # Run Apriori with slightly lower support due to smaller dataset\n",
    "    upper_itemsets = apriori(upper_df, min_support=0.08, use_colnames=True)\n",
    "    if len(upper_itemsets) > 0:\n",
    "        upper_rules = association_rules(upper_itemsets, metric='confidence', min_threshold=0.5)\n",
    "        upper_rules = upper_rules[upper_rules['lift'] >= 1.0]\n",
    "        \n",
    "        print(f\"UPPER/TORSO WORKOUT RULES ({len(upper_sessions)} sessions)\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        for idx, row in upper_rules.sort_values('lift', ascending=False).head(10).iterrows():\n",
    "            antecedent = ', '.join(list(row['antecedents']))\n",
    "            consequent = ', '.join(list(row['consequents']))\n",
    "            print(f\"\\n{antecedent} → {consequent}\")\n",
    "            print(f\"  Support: {row['support']:.3f} | Confidence: {row['confidence']:.3f} | Lift: {row['lift']:.2f}\")\n",
    "    else:\n",
    "        print(\"No frequent itemsets found for Upper workouts with current support threshold\")\n",
    "else:\n",
    "    print(f\"Not enough Upper workout sessions for analysis ({len(upper_transactions)} found)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze \"Legs/Limbs\" workout associations\n",
    "legs_sessions = sessions_df[sessions_df['workout_type'].str.contains('Leg|Limb|Lower', case=False, na=False)]\n",
    "legs_transactions = legs_sessions['exercises_list'].apply(parse_exercises).tolist()\n",
    "\n",
    "if len(legs_transactions) >= 20:\n",
    "    te_legs = TransactionEncoder()\n",
    "    te_legs_array = te_legs.fit_transform(legs_transactions)\n",
    "    legs_df = pd.DataFrame(te_legs_array, columns=te_legs.columns_)\n",
    "    \n",
    "    legs_itemsets = apriori(legs_df, min_support=0.08, use_colnames=True)\n",
    "    if len(legs_itemsets) > 0:\n",
    "        legs_rules = association_rules(legs_itemsets, metric='confidence', min_threshold=0.5)\n",
    "        legs_rules = legs_rules[legs_rules['lift'] >= 1.0]\n",
    "        \n",
    "        print(f\"LEGS/LIMBS WORKOUT RULES ({len(legs_sessions)} sessions)\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        for idx, row in legs_rules.sort_values('lift', ascending=False).head(10).iterrows():\n",
    "            antecedent = ', '.join(list(row['antecedents']))\n",
    "            consequent = ', '.join(list(row['consequents']))\n",
    "            print(f\"\\n{antecedent} → {consequent}\")\n",
    "            print(f\"  Support: {row['support']:.3f} | Confidence: {row['confidence']:.3f} | Lift: {row['lift']:.2f}\")\n",
    "    else:\n",
    "        print(\"No frequent itemsets found for Legs workouts with current support threshold\")\n",
    "else:\n",
    "    print(f\"Not enough Legs workout sessions for analysis ({len(legs_transactions)} found)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Findings and Interpretation\n",
    "\n",
    "### 6.1 Summary Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final summary\n",
    "print(\"=\" * 70)\n",
    "print(\"M2 ANALYSIS SUMMARY\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(f\"\\n--- Dataset Overview ---\")\n",
    "print(f\"Training period: {sessions_df['date'].min().date()} to {sessions_df['date'].max().date()}\")\n",
    "print(f\"Total sessions: {len(sessions_df)}\")\n",
    "print(f\"Total sets: {len(sets_df)}\")\n",
    "print(f\"Unique exercises: {sets_df['exercise_standard'].nunique()}\")\n",
    "print(f\"Unique workout types: {sessions_df['workout_type'].nunique()}\")\n",
    "\n",
    "print(f\"\\n--- Data Quality ---\")\n",
    "print(f\"Missing weight values: {sets_df['weight_lbs'].isnull().sum()} ({sets_df['weight_lbs'].isnull().mean()*100:.1f}%)\")\n",
    "print(f\"Missing rep values: {sets_df['reps'].isnull().sum()} ({sets_df['reps'].isnull().mean()*100:.1f}%)\")\n",
    "print(f\"Synthetic data: {sessions_df['is_synthetic'].sum()} sessions ({sessions_df['is_synthetic'].mean()*100:.1f}%)\")\n",
    "\n",
    "print(f\"\\n--- Association Rule Mining Results ---\")\n",
    "print(f\"Parameters: min_support={MIN_SUPPORT}, min_confidence={MIN_CONFIDENCE}, min_lift={MIN_LIFT}\")\n",
    "print(f\"Frequent itemsets discovered: {len(frequent_itemsets)}\")\n",
    "print(f\"Association rules generated: {len(rules)}\")\n",
    "print(f\"Average lift of rules: {rules['lift'].mean():.2f}\")\n",
    "print(f\"Max lift: {rules['lift'].max():.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 Key Findings\n",
    "\n",
    "**Finding 1: Exercise Pairing Patterns**\n",
    "\n",
    "[Interpret your top rules here - what exercises frequently appear together?]\n",
    "\n",
    "**Finding 2: Workout Structure**\n",
    "\n",
    "[Discuss what the associations reveal about how workouts are structured]\n",
    "\n",
    "**Finding 3: Muscle Group Synergies**\n",
    "\n",
    "[Discuss any patterns related to muscle group pairings]\n",
    "\n",
    "### 6.3 Limitations\n",
    "\n",
    "1. The analysis treats all sessions equally, regardless of whether workout types changed over time\n",
    "2. Association rules don't capture the *order* of exercises within a session\n",
    "3. Some exercises may be paired due to equipment availability rather than training principles\n",
    "\n",
    "### 6.4 Next Steps for M3\n",
    "\n",
    "1. **Clustering (Q2):** Apply K-Means to identify distinct training phases based on volume, intensity, and exercise selection\n",
    "2. **Anomaly Detection (Q3):** Use Isolation Forest to identify unusually high or low performance sessions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export rules for reference\n",
    "rules_export = rules.copy()\n",
    "rules_export['antecedents'] = rules_export['antecedents'].apply(lambda x: ', '.join(list(x)))\n",
    "rules_export['consequents'] = rules_export['consequents'].apply(lambda x: ', '.join(list(x)))\n",
    "rules_export.to_csv('association_rules_output.csv', index=False)\n",
    "print(\"Rules exported to 'association_rules_output.csv'\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
